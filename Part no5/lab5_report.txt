****Nikitakis Panagiotis
****AEM: 1717
****pnikitakis@uth.gr
****23/12/2016
****HPC CE421 lab5 

Για αρχείο εισόδου χρησιμοποίησα το binary texture17965 γιατί έχει τον μεγαλύτερο αριθμό συντεταγμένων και αντικειμένων και πειραματίστηκα με 20 centroids. Μετά από κάθε αλλαγή έλεγχα τα αρχεία .clusters .membership για την ορθότητα και τελικά όλες οι αλλαγές που έκανα ήταν ορθές, με μικρές διαφορές στο 3 ή 4 δεκαδικό ψηφίο στις συντεταγμένες των clusters.  


1η αλλαγή(γραμμη 40): 
Μετά από το profiling, 
πήγα να βελτιώσω αρχικά την γραμμή 40 το loop. Τρέχοντας static, dynamic, dynamic με διαφορετικά chunk size και guided, για 1,2,6 και 12 threads, ο χρόνος χειροτερεύει πάρα πολύ με σύγκριση τον σειριακό κώδικα ενός thread. Αυτό οφείλεται στο ότι ξοδεύουμε πολύ χρόνο στο να δημιουργήσουμε τα threads και γίνεται λίγη δουλειά. Καθώς το loop τρέει μέχρι το numCords που δεν ξεπερνάει το 20 στα αρχεία εισόδου. Μια μικρή βελτίωση είναι να δηλώσουμε το (#pragma omp parallel) στην καλούσα συνάρτηση για να δημιουργηθούν μία φορά τα threads και όχι κάθε φορά που καλέιται η euclid_dist. Μεταξύ των δίαφορων schedule δεν υπαρχει μεγάλη διαφορά, αυτό φαίνεται στο αρχειο(change1.png) με τους χρονους. 

2η αλλαγή(γραμμη 60):
Μετά πήγα να αλλάξω στην γραμμή 60. 
Αρχικά ορίζω την μεταβλητή min_dist να την εκτελέσει μόνο ένα thread μία φορά και να είναι shared για να μπορούν να τη διαβάσουν όλοι. Έπειτα παραλληλοποιώ το loop που τρέχει μέχρι των αριθμό των clusters. Σε πρώτη φάση ΔΕΝ κράτησα τη προηγούμενη αλλαγή στη **γραμμή 40**. Δοκιμάζοντας με διαφορετικά schedule παρατήρησα ότι δεν διαφέρουν σε μεγάλο βαθμό μεταξύ τους. Για αυτό πειραματίστηκα μετά με το chunk size και κατέληξα πως ο χρόνος βελτιώνεται καθώς αυξάνεται το chunk μέχρι το 20 και μετά μένει σταθερός. Φαίνεται στην εικόνα(change2.png). Τέλος πρόσθεσα ενα (omp cirtical) για να να μην υπάρχει πρόβλημα όταν προσπαθούν όλα τα threads να αλλάξουν το min.
Σε δεύτερη φάση, πρόσθεσα και τη προηγούμενη αλλαγή και παρατήρησα ότι ο χρόνος φράσσεται από το κάτω όριο της 1ης αλλαγής αρα είναι χειρότερος και για αυτό κρατάω μόνο τη **γραμμη 60**. Τέλος, όπως και πριν, πρόσθεσα το (#pragma omp parallel) στην καλούσα συνάρτηση και ο χρόνος μειώθηκε δραματικά λόγω του ότι φτιάχνουμε και καταστρέφουμε μία μόνο φορά τα threads. Εικόνα (change2.png).

3η αλλαγή(γραμμη 118, 121, 134):
Έπειτα παραλληλοποίησα το loop που περνάει όλα τα σημεία και τα αναθέτει σε cluster. Χρειάστηκε (omp critical) για το newClusters[] και (omp reduction) για το delta. Δοκίμασα αρχικά μαζί με την 2η αλλαγή, χωρίς την 1η και οι χρόνοι καλυτέρεψαν. Έπειτα έτρεξα χωρίς καμία από τις προηγούμενες αλλαγές και υπήρξε μεγάλη διαφορά. (change3.png). Αυτό οφείλεται στο γεγονός πως όταν έχουμε πολλαπλά threads και σε καθένα από αυτά καλούμε επιπλέον threads, δεν υπάρχουν αρκετά διαθέσιμα και μπαίνουν σε ουρά για να κάνουν τα tasks.

4η αλλαγή(γραμμη 143, 152):
Μια μικρή προσθήκη για μοιράσω το loop στα threads, αλλά δεν φαίνεται μεγάλη διαφορά στους χρόνους. Λόγω του ότι με 20 clusters και maximum 20 συντεταγμένες, το κάθε threads δεν έχει πολλή δουλειά να κάνει. Εικόνα (change4.png). 

5η αλλαγή(γραμμή 105):
Μία μικρή προσθήκη για να τελειώσει πιο γρήγορα η αρχικοποίηση του membership[].

Ο καλύτερος χρόνος που έπιασα ήταν για 2 threads: 0.407450127602 με τυπική απόκλιση 0.000637018501, ενώ ο αρχικός σειριακός κώδικας ήταν στα: 0.604313421249 με sd = 0.026266734788.
Μία γενική παρατήρηση είναι πως το static και dynamic scheduling δεν έχουν μεγάλες διαφορές στον χρόνο. Αυτό οφείλειται στο ότι η δουλειά που αναθέτουμε στα threads είναι ίσου μεγέθους, επομένως το dynamic και guided δεν χρειάζονται για να εξισορροπήσουν το φορτίο εργασίας. 
Τελικά η βελτίωση δεν ήταν σημαντική με σχέση τον σειριακό κώδικα και ίσως οφείλεται στο ότι έβαλα 20 centroids. 
Επίσης από τα αποτελέσματα, βλέπω πως όταν έχουμε 2 threads είναι ελαφρά πιο αποδοτικό από οταν έχουμε περισσότερα. Για αυτό στο τέλος δοκίμασα nested parallelism. Αρα στη γραμμή 120 αντί για 12 θα χρησιμοποιήσω 2 threads. Τα οποία με την σειρά τους όταν φτάσουν στη γραμμή 60, το καθένα θα δημιουργήσει άλλο ένα(επειδή είναι ήδη το master το ένα από τα δύο). Τα οποία στη συνέχεια θα μπουν στη euclid_dist_2 και στη γραμμή 38 θα δημιουργηθούν και άλλα. Συνολικά δηλαδή 8 threads. Στη γραμμή 102, 103 είναι οι εντολές που χρειάζονται για το nested parallelism.
Παρόλα αυτά ο χρόνος είναι χειρότερος από ότι μετά την 5η αλλαγή: 6.549622702599, sd = 0.106305494669.
Για τον compiler χρησιμοποίησα icc -qopenmp -g -traceback -fast -DNDEBUG









