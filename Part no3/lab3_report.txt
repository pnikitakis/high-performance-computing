****Nikitakis Panagiotis
****AEM: 1717
****pnikitakis@uth.gr
****18/11/2016
****HPC CE421 lab3 

*deviceQ.txt: Τα αποτελέσματα από το deviceQuery() στον Mars2.
*conc6.cu: Το αρχείο από το lab2 με divergences, το χρησιμοποιώ για να πάρω τους χρόνους χωρίς tiling.
*lab3.cu: Ο καινούργιος κώδικας όπου το μέγεθος του tile είναι ίδιο με το μέγεθος του thread block.
*lab3b.cu: Ο κώδικας όπου το tile είναι διαφορετικό μέγεθος από το thread block.
*ptxasOpt.png & ptxasOpt2.png:  Οι ποσότητες των πόρων για τα lab3.cu και lab3b.cu αντίστοιχα.


Στον πρώτο κώδικα αφού έπρεπε να το μέγεθος του tile να είναι ίσο με μέγεθος του thread block άρα 1024, έκανα τα thread blocks στον Row kernel να καταλαμβάνουν όσα περισσότερα οριζόντια στοιχεία γίνεται γιατί οι πράξεις γίνονται οριζόντια. Ενώ αντίστοιχα στον Col kernel έκανα κάθετα τα thread blocks γιατί οι πράξεις γίνονται κατακόρυφα. Με αυτό το τρόπο χρησιμοποιώ στο μέγιστο τα στοιχεία που φέρνω. Ένα πρόχειρο σχεδιάγραμμα φαίνεται στο (tile.pdf).
Στην εικόνα (dif_tiling_float.jpg) βλέπουμε τη διαφορά όταν χρησιμοποιούμε shared memory με tile size = 1024 με αντίθεση όταν παίρνουμε τα στοιχεία κατευθείαν από την global.
 
Χρησιμοποιώντας την παράμετρο (-ptxas-options=-v) βλέπω πως η κάθε shared memory έχει 4096 bytes (4 float * 1024 tile size) ενώ τα υπόλοιπα 45KB μένουν αχρησιμοποίητα.
Το μέγιστο tile θα ήταν (49KB / 4Β) = 12.250 στοιχεία του πίνακα.
Από το προηγούμενο lab είδα πως όταν έχουμε πίνακα μεγαλύτερο από 8.192χ8.192 δεν υπάρχει αρκετός χώρος πάνω στη GPU, άρα max(imageW) = 8.192.
Επομένως στον δεύτερο κώδικα προσπάθησα να φέρνω 8.192 στοιχεία ανά thread block για να έχω πάλι οριζόντια(και κάθετη αντίστοιχα) χρησιμοποίηση των στοιχείων αλλά και για να αναθέσω περισσότερη δουλειά στο κάθε thread block γιατί τώρα θα περνάει από περισσότερα στοιχεία. Ένα πρόχειρο σχεδιάγραμμα φαίνεται στο (tile2.pdf).

Επειδή δεν κατάφερα να υλοποιήσω σωστά τον δεύτερο κώδικα, δεν έχω στατιστικά στοιχεία για το πόσο πιο γρήγηρος είναι από την προηγούμενη υλοποίηση. Κάνοντας μία θεωρητική "μαντεψιά" θα έλεγα ότι είναι 6χ φορές πιο γρήγορος από τον lab3.cu. Αυτό γιατί φέρνουμε 8χ περισσότερα στοιχεία στην shared memory και επειδή θα έχουμε κάποια παραπάνω overhead γιατί το κάθε thread block έχει περισσότερη δουλεία να κάνει(δουλεύει περισσότερα στοιχεία από οτι πριν). 

για την 2η υλοποιήση που θεωρητικά έχω tile size = 8.192:
Για πίνακα με μέγεθος 1024χ1024 έχουμε τα εξής:
Το κάθε thread φέρνει 8 στοιχεία από την global στην shared, μετά διαβάζει αυτά τα 8 στοιχεία και διαβάζει και 16 από την constant που έχει το φίλτρο.
Οι πράξεις που κάνει είναι : 1024 επαναλήψεις από 2 πράξεις στην κάθε μία. Άρα για κάθε thread έχουμε λόγο 256:1 πράξεις προς προσπέλαση μνήμης, κάτι το οποιό ειναι μακράν καλύτερο από το προηγούμενο lab όπου είχα 1:1 αναλογία.
 
Χρησιμοποιώντας double μειώνεται το tile που μπορούμε να κάνουμε καθώς χρειαζόμαστε 8 bytes ανά στοιχείο αντί για 4. Άρα (49KB / 8B) = 6.125 στοιχεία χωράνε στην shared memory. Άρα το μέγιστο μέγεθος που μπορώ να έχω στην 2η υλοποίηση μου για το tile είναι 4096 στοιχεία στην shared. Αυτό σημαίνει οτι το πρόγραμμος θα είναι περίπου 2χ πιο αργό από ότι με floats. Αυτά πάντα θεωρητικά γιατί δεν κατάφερα το δεύτερο πρόγραμμα να τρέχει σωστά.
Με σύγκριση τους χρόνους από το προηγούμενο lab και της 1η υλοποιησης που έκανα με 1024 tile στην shared memory φαίνεται στο (dif_tiling_double.jpg). Παρατηρώ πως όσο αυξάνεται το μέγεθος του πίνακα τόσο μεγαλώνει η διαφορά του χρόνου όσο αφορά floats και doubles.



